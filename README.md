# Overview:

* Supervised learning:
  * Accident prediction with: Neural Network versus ML algorithm
* LLM
  * Chatbot
* Unity
  * Car simulator
    <br><br>
    By using the Unity Game Engine and the MLAgents package, we have developed a car simulator that can be used to train a reinforcement learning model to drive a car. The simulator is a 3D environment where the car has to follow a pre-defined road, which the model has to learn to do by itself by using only the car's "Ray Perception Sensors" as input.
    <br><br>
    Below, the car can be seen with its 2x15 ray-sensors activated and visualized:
    <br><br>
    ![car_rayperception.png](images/car_rayperception.png)
    <br><br>
    Each sensor generates a value of 1 if it detects an object, or 0 if it doesn't, as well as the distance to where the ray hit the object. The model then has to learn to interpret these values and use them to steer the car in the right direction.
    <br>
    As the learning method is reinforcement learning, the model is rewarded for staying on the road, and penalized for driving off the road. The model is trained using the Proximal Policy Optimization (PPO) algorithm, which is a type of reinforcement learning algorithm that is well-suited for continuous action spaces.
    <br><br>
    Training a model like this can take a long time, and a lot of resources over multiple generations. For this reason, we chose to generate data for the car with 2 ray-sensors with 15 rays each, as this generates an input vector with a size of 60. In comparison, an RGB camera-sensor with even a low resolution of 256x256 would produce an input vector with the size of 196,608. 
    <br>
    This input vector is generated by each instance of the environment, and has to be processed by the model in each step of the training process. By using the ray-sensors, we can reduce the size of the input vector by a factor of over 3,000, which greatly reduces the amount of resources needed to train the model.
    <br>
    Below is an example of the progress achieved by a training process which used 158 instances of the environment to collectively train the model for 5,000,000 steps. The clip shows a 13 minute training session boiled down to 30 seconds:
    <br><br>
    ![CarNN.gif](images/CarNN.gif) ☹️
    <br><br>
    <br><br>
  
  * Traffic signs
    * Data generation 
    <br><br>
    At first, we used the traffic sign dataset from the German Traffic Sign Recognition Benchmark (GTSRB) as the data for training a model to recognize traffic signs. However, the dataset was too small (around 900 images in total) to train a model that could generalize well to real-world images. To solve this problem, we utilized the Unity Game Engine to generate synthetic data.
    <br>
    The synthetic data generation required 2 main parts. 
    <br><br>
    The first part is a collection of background images from a relevant environment, in this case urban and/or rural traffic environments.
    <br>
    An example of such an image could be like the following, which was simply found on google:
    <br><br>
    ![image1174.jpg](images/image1174.jpg) ☹️
    <br><br>
    The second part is a collection of traffic sign "prefabs", which are blueprints of 3D objects that unity can manipulate and apply methods to. This includes scaling, rotating, and translating the objects, as well as applying textures to them.
    <br>
    These 3D objects are a combination of simple shapes, and textures that are applied to them. The textures in our case were the following collection of images:
    <br><br>
    ![traffic_sign_collection.png](images/traffic_sign_collection.png) ☹️
    <br><br>
    We used Unity to combine these parts in random ways, and thereby managed to develop a solution which could generate any given amount of synthetic images, which gives us an unlimited amount of training data for our traffic sign recognition model.
    <br>
    An example of such a generated image could be the following:
    <br><br>
    ![step0.camera.png](images/step0.camera.png) ☹️
    <br><br> 
    
    * Prediction
    <br><br>
    For prediction, we used the YOLO model, which is a pre-trained CNN model that can detect objects in images. We trained the model on the synthetic data we generated in Unity, and then used it to predict traffic signs in real images.
    <br>
    During the training process, the YOLO training script outputs a collection of validation images with the predicted bounding boxes drawn on them, which is used to get a general idea of the models accuracy. An example of such an image could be the following:
    <br><br>
    ![val_batch2_pred.jpg](images/val_batch2_pred.jpg) ☹️
    <br><br>
    In that validation-batch, the model seems to achieve a high accuracy, as it predicts most of the taffic signs correctly with a high certainty.
    <br><br>
    We also tested the model on real images, and it seems to perform well on those as well. An example of the output of such a prediction could be the following:
    <br><br>
    ![trafik9.jpg](images/trafik9.jpg) ☹️
    <br><br>
    Our end goal is to combine this model with the car simulator, so that the car can detect traffic signs in real-time, and react to them accordingly.
    <br><br>

* Streamlit
  * UI
  * Map with accidents
  * Manual prediction
    * User input
      * With output from LLM   
